"""
@ê²½ë¡œ: utils/datasets/rag_cot_en_ko.py
@ì„¤ëª…: HuggingFaceì˜ RAG-COT-En_KO ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì €ì¥/í™•ì¸í•  ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤
@ëª…ë ¹ì–´: python utils/datasets/rag_cot_en_ko.py
"""
import random
import json
import os
import sys
from pathlib import Path
from datasets import load_dataset

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ê²½ë¡œ ì„¤ì • (import ì‹œì—ëŠ” ì‹¤í–‰ ì•ˆ ë¨)
if __name__ == "__main__":
    project_root = Path(__file__).resolve().parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    os.environ['PYTHONPATH'] = str(project_root)

from utils.log.logging import logger

class RagCotEnKoDataset:
    """RAG-COT-En_KO ë°ì´í„°ì…‹ ê´€ë¦¬ í´ë˜ìŠ¤ (ì˜ì–´ ì»¨í…ìŠ¤íŠ¸, í•œêµ­ì–´ ì§ˆë¬¸/ë‹µë³€)"""
    
    def __init__(self, split="train", max_samples=None, streaming=False):
        """
        RAG-COT-En_KO ë°ì´í„°ì…‹ ë¡œë“œ
        split: 'train' (í•™ìŠµìš©), 'validation' (í‰ê°€ìš©), 'test' (í…ŒìŠ¤íŠ¸ìš©)
        max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
        streaming: ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (í° ë°ì´í„°ì…‹ìš©)
        """
        logger.info(f"Loading RAG-COT-En_KO dataset ({split})...")
        if max_samples:
            logger.info(f"Dataset size limited to: {max_samples} samples")
        if streaming:
            logger.info("Using streaming mode for large dataset")
        
        # HuggingFace ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì •ë³´ ë¡œê¹…
        dataset_name = "jaeyong2/RAG-COT-En_KO"
        logger.info(f"Dataset identifier: {dataset_name}")
        logger.info(f"HuggingFace URL: https://huggingface.co/datasets/{dataset_name}")

        try:
            # ë°ì´í„°ì…‹ ë¡œë“œ ì „ ìºì‹œ ê²½ë¡œ í™•ì¸ (datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ë³„ í˜¸í™˜ì„±)
            try:
                from datasets.utils.file_utils import HF_CACHE_HOME
                cache_dir = HF_CACHE_HOME
            except ImportError:
                try:
                    from datasets import config
                    cache_dir = config.HF_DATASETS_CACHE
                except (ImportError, AttributeError):
                    cache_dir = os.path.expanduser("~/.cache/huggingface/datasets")
            
            logger.info(f"Local cache directory: {cache_dir}")
            
            #  ----------------------------- 1. ë°ì´í„°ì…‹ ë¡œë“œ (ìµœì í™”) ----------------------------- 
            if streaming:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ì§€ë§Œ ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ
                self.dataset = load_dataset(dataset_name, split=split, streaming=True)
                logger.info("Dataset loaded in streaming mode")
                
                # ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” ìƒ˜í”Œ ìˆ˜ë¥¼ ì œí•œí•˜ê¸° ìœ„í•´ take() ì‚¬ìš©
                if max_samples:
                    self.dataset = self.dataset.take(max_samples)
                    logger.info(f"Limited to {max_samples} samples in streaming mode")
                    
            else:
                # ì¼ë°˜ ëª¨ë“œ: ì „ì²´ ë¡œë“œ í›„ í•„ìš”ì‹œ ìƒ˜í”Œë§
                self.dataset = load_dataset(dataset_name, split=split)
                original_size = len(self.dataset)
                logger.info(f"[CHECK] RAG-COT-En_KO dataset loaded successfully. Original size: {original_size}")
                
                # í¬ê¸° ì œí•œì´ ìˆìœ¼ë©´ ë¯¸ë¦¬ ìƒ˜í”Œë§
                if max_samples and max_samples < original_size:
                    indices = random.sample(range(original_size), max_samples)
                    self.dataset = self.dataset.select(indices)
                    logger.info(f"[CHECK] Pre-sampled to {max_samples} samples from {original_size}")
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ í•„í„°ë§ ì ìš© (ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ìŠ¤í‚µ)
            if not streaming:
                #  ---------- 2. ì „ì²˜ë¦¬: ë¯¼ê°í•œ ì£¼ì œ í•„í„°ë§ (Azure Content Filter ë°©ì§€) ---------- 
                logger.info("Starting Preprocessing: Filtering sensitive topics...")
                
                # ì˜ì–´ì™€ í•œêµ­ì–´ ëª¨ë‘ ê³ ë ¤í•œ í•„í„°ë§ ë‹¨ì–´
                forbidden_words_ko = ["ì •ì¹˜", "ì„ ê±°", "ëŒ€í†µë ¹", "ì‹œìœ„", "í­ë ¥", "ì‚´ì¸", "ë²”ì£„", "ì „ìŸ", "ì‚¬ë§", "í”¼í•´", "ë¶í•œ", "ë¯¸ì‚¬ì¼", "ì •ì¹˜ì ", "ì „í›„ë¯¼ì£¼ì£¼ì˜"]
                forbidden_words_en = ["politics", "election", "president", "protest", "violence", "murder", "crime", "war", "death", "damage", "nuclear", "missile", "political", "democracy"]
                forbidden_words = forbidden_words_ko + forbidden_words_en
                
                def is_safe_content(example):
                    # RAG-COT-En_KO ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ í•„ë“œ ì¶”ì¶œ
                    text_sources = [
                        example.get('context', ''),  # ì˜ì–´ ì»¨í…ìŠ¤íŠ¸
                        example.get('question', ''),  # í•œêµ­ì–´ ì§ˆë¬¸
                        example.get('answer', ''),   # í•œêµ­ì–´ ë‹µë³€
                        example.get('ko_question', ''),  # ê°€ëŠ¥í•œ í•„ë“œëª…
                        example.get('ko_answer', ''),    # ê°€ëŠ¥í•œ í•„ë“œëª…
                        example.get('en_context', ''),   # ê°€ëŠ¥í•œ í•„ë“œëª…
                        str(example.get('reasoning', ''))  # COT reasoning ë¶€ë¶„
                    ]
                    combined_text = " ".join([str(t) for t in text_sources if t])
                    
                    for word in forbidden_words:
                        if word in combined_text.lower():
                            return False
                    return True

                # í•„í„°ë§ ì ìš©
                pre_filter_size = len(self.dataset) if hasattr(self.dataset, '__len__') else 0
                self.dataset = self.dataset.filter(is_safe_content)
                post_filter_size = len(self.dataset) if hasattr(self.dataset, '__len__') else 0
                if pre_filter_size > 0:
                    logger.info(f"[CHECK] Filtered dataset size: {post_filter_size} (removed {pre_filter_size - post_filter_size} items)")
                else:
                    logger.info(f"[CHECK] Filtered dataset completed")
            else:
                logger.info("Skipping filtering in streaming mode for memory efficiency")

            # ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì •ë³´ ë¡œê¹…
            if hasattr(self.dataset, 'info'):
                logger.info(f"Dataset info: {self.dataset.info}")
            if hasattr(self.dataset, 'builder_name'):
                logger.info(f"Builder name: {self.dataset.builder_name}")
            if hasattr(self.dataset, 'config_name'):
                logger.info(f"Config name: {self.dataset.config_name}")
                
        except Exception as e:
            logger.error(f"Failed to load RAG-COT-En_KO dataset: {e}")
            raise

    def get_cache_info(self):
        """ë°ì´í„°ì…‹ ìºì‹œ ì •ë³´ë¥¼ ë°˜í™˜"""
        try:
            # datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ë³„ í˜¸í™˜ì„± ì²˜ë¦¬
            try:
                from datasets.utils.file_utils import HF_CACHE_HOME
                cache_home = HF_CACHE_HOME
            except ImportError:
                try:
                    from datasets import config
                    cache_home = config.HF_DATASETS_CACHE
                except (ImportError, AttributeError):
                    cache_home = os.path.expanduser("~/.cache/huggingface/datasets")
            
            cache_info = {
                "cache_home": cache_home,
                "cache_exists": os.path.exists(cache_home),
                "dataset_name": "jaeyong2/RAG-COT-En_KO"
            }
            
            if hasattr(self.dataset, 'cache_files'):
                cache_info["cache_files"] = self.dataset.cache_files
            
            logger.info(f"Cache info: {cache_info}")
            return cache_info
            
        except Exception as e:
            logger.error(f"Failed to get cache info: {e}")
            return {}

    def get_random_samples(self, n=1):
        """
        ëœë¤í•˜ê²Œ nê°œì˜ (ì§ˆë¬¸, ì •ë‹µ, ì§€ë¬¸) ìƒ˜í”Œì„ ë°˜í™˜
        ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œëŠ” ìˆœì°¨ì ìœ¼ë¡œ ìƒ˜í”Œ ì¶”ì¶œ
        
        Returns:
            list of tuple: [(question, answer, context), ...]
        """
        if n <= 0:
            raise ValueError("n must be positive")
        
        results = []
        
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì¸ì§€ í™•ì¸
        is_streaming = hasattr(self.dataset, 'take')
        
        if is_streaming:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: ìˆœì°¨ì ìœ¼ë¡œ nê°œ ì¶”ì¶œ
            logger.info(f"Extracting {n} samples from streaming dataset...")
            count = 0
            for item in self.dataset:
                if count >= n:
                    break
                    
                try:
                    question, answer, context = self._extract_fields(item)
                    results.append((question, answer, context))
                    count += 1
                except Exception as e:
                    logger.warning(f"Error processing streaming sample {count}: {e}")
                    continue
                    
        else:
            # ì¼ë°˜ ëª¨ë“œ: ëœë¤ ìƒ˜í”Œë§
            dataset_size = len(self.dataset)
            if n > dataset_size:
                logger.warning(f"Requested {n} samples but dataset has only {dataset_size} items. Using all available.")
                n = dataset_size
                
            indices = random.sample(range(dataset_size), n)
            
            for idx in indices:
                item = self.dataset[idx]
                try:
                    question, answer, context = self._extract_fields(item)
                    results.append((question, answer, context))
                except Exception as e:
                    logger.warning(f"Error processing sample {idx}: {e}")
                    results.append(("", "", ""))
        
        return results

    def _extract_fields(self, item):
        """ë°ì´í„° í•­ëª©ì—ì„œ question, answer, context í•„ë“œ ì¶”ì¶œ"""
        # ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ëŠ” í•„ë“œëª… ì‚¬ìš©
        question_candidates = ['Question', 'question', 'ko_question', 'query', 'input']
        answer_candidates = ['Final Answer', 'answer', 'ko_answer', 'output', 'response']
        context_candidates = ['context', 'en_context', 'passage', 'document']
        thinking_candidates = ['Thinking', 'thinking', 'reasoning', 'cot', 'chain_of_thought']
        
        question = ""
        answer = ""
        context = ""
        thinking = ""
        
        # ì§ˆë¬¸ í•„ë“œ ì°¾ê¸°
        for q_field in question_candidates:
            if q_field in item and item[q_field]:
                question = str(item[q_field])
                break
        
        # ë‹µë³€ í•„ë“œ ì°¾ê¸° (Final Answer ìš°ì„ )
        for a_field in answer_candidates:
            if a_field in item and item[a_field]:
                answer = str(item[a_field])
                break
        
        # ì»¨í…ìŠ¤íŠ¸ í•„ë“œ ì°¾ê¸°
        for c_field in context_candidates:
            if c_field in item and item[c_field]:
                context = str(item[c_field])
                break
        
        # Thinking(COT reasoning) í•„ë“œ ì°¾ê¸°
        for t_field in thinking_candidates:
            if t_field in item and item[t_field]:
                thinking = str(item[t_field])
                break
        
        # Chain-of-Thoughtê°€ ìˆë‹¤ë©´ ë‹µë³€ì— í¬í•¨ (ë” í’ë¶€í•œ í•™ìŠµì„ ìœ„í•´)
        if thinking and thinking not in answer:
            # Thinkingì´ ë§¤ìš° ê¸¸ë©´ ìš”ì•½ëœ ë¶€ë¶„ë§Œ í¬í•¨
            if len(thinking) > 200:
                thinking_summary = thinking[:200] + "..."
                answer = f"[ì‚¬ê³  ê³¼ì •: {thinking_summary}]\n\n{answer}" if answer else f"[ì‚¬ê³  ê³¼ì •: {thinking_summary}]"
            else:
                answer = f"[ì‚¬ê³  ê³¼ì •: {thinking}]\n\n{answer}" if answer else f"[ì‚¬ê³  ê³¼ì •: {thinking}]"
        
        # í•„ë“œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ë¡œê¹…
        if not question and not answer and not context:
            logger.warning(f"Could not extract fields from item. Available keys: {list(item.keys())}")
            
        return question, answer, context

    def get_fixed_sample(self, index: int = 0) -> tuple:
        """
        íŠ¹ì • ì¸ë±ìŠ¤ì˜ ìƒ˜í”Œ ë°˜í™˜ (ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸ìš©)
        ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ
        
        Args:
            index (int): ë°ì´í„°ì…‹ ë‚´ íŠ¹ì • ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)
            
        Returns:
            tuple: (ì§ˆë¬¸, ì •ë‹µ, ì§€ë¬¸)
        """
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì¸ì§€ í™•ì¸
        is_streaming = hasattr(self.dataset, 'take')
        if is_streaming:
            logger.error("get_fixed_sample is not supported in streaming mode")
            return "", "", ""
            
        if index >= len(self.dataset):
            logger.warning(f"Index {index} is out of range. Dataset size: {len(self.dataset)}")
            return "", "", ""
        
        sample = self.dataset[index]
        
        try:
            question, answer, context = self._extract_fields(sample)
            return question, answer, context
            
        except Exception as e:
            logger.warning(f"Error processing fixed sample {index}: {e}")
            logger.warning(f"Sample keys: {list(sample.keys())}")
            return "", "", ""

    def save_random_samples(self, num_samples: int = 10, output_format: str = "json") -> str:
        """
        ëœë¤ ìƒ˜í”Œë“¤ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë°ì´í„° êµ¬ì¡° í™•ì¸
        
        Args:
            num_samples (int): ì €ì¥í•  ìƒ˜í”Œ ê°œìˆ˜ (ê¸°ë³¸: 10ê°œ)
            output_format (str): "json" ë˜ëŠ” "csv"
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if not self.dataset:
            raise ValueError("Dataset not loaded.")
        
        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì¸ì§€ í™•ì¸
        is_streaming = hasattr(self.dataset, 'take')
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ samples í´ë” ìƒì„±
        current_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        samples_dir = Path(current_dir) / "datasets" / "samples"
        samples_dir.mkdir(parents=True, exist_ok=True)
        
        # ëœë¤ ìƒ˜í”Œ ìˆ˜ì§‘
        samples = []
        
        if is_streaming:
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: ìˆœì°¨ì ìœ¼ë¡œ ìƒ˜í”Œ ìˆ˜ì§‘
            logger.info(f"Collecting {num_samples} samples from streaming dataset...")
            count = 0
            for sample in self.dataset:
                if count >= num_samples:
                    break
                
                processed_sample = {
                    "index": count,
                    "original_index": count,  # ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” ìˆœì°¨ì 
                }
                
                # ëª¨ë“  í•„ë“œë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€
                for key, value in sample.items():
                    processed_sample[key] = value
                
                samples.append({
                    "index": count,
                    "processed_sample": processed_sample,
                    "raw_keys": list(sample.keys())
                })
                count += 1
        else:
            # ì¼ë°˜ ëª¨ë“œ: ëœë¤ ìƒ˜í”Œë§
            dataset_size = len(self.dataset)
            indices = random.sample(range(dataset_size), min(num_samples, dataset_size))
            
            for i, idx in enumerate(indices):
                sample = self.dataset[idx]
                
                processed_sample = {
                    "index": i,
                    "original_index": idx,
                }
                
                # ëª¨ë“  í•„ë“œë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€
                for key, value in sample.items():
                    processed_sample[key] = value
                
                samples.append({
                    "index": i,
                    "processed_sample": processed_sample,
                    "raw_keys": list(sample.keys())
                })
        
        if output_format.lower() == "json":
            output_file = samples_dir / "rag_cot_en_ko_samples.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(samples, f, ensure_ascii=False, indent=2)
                
        elif output_format.lower() == "csv":
            import csv
            output_file = samples_dir / "rag_cot_en_ko_samples.csv"
            
            if samples:
                # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ í‚¤ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í—¤ë” ìƒì„±
                first_sample = samples[0]['processed_sample']
                fieldnames = ['index'] + [k for k in first_sample.keys() if k != 'index']
                
                with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    for s in samples:
                        row_data = {}
                        p = s['processed_sample']
                        for field in fieldnames:
                            value = p.get(field, '')
                            # ë³µì¡í•œ ê°ì²´ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜
                            if isinstance(value, (list, dict)):
                                value = str(value)
                            row_data[field] = value
                        writer.writerow(row_data)
        else:
            raise ValueError("output_format must be 'json' or 'csv'")
        
        logger.info(f"Saved {len(samples)} samples to {output_file}")
        
        # ë°ì´í„° êµ¬ì¡° ë¡œê·¸ ì¶œë ¥
        if samples:
            logger.info(f"Sample structure analysis:")
            first_sample = samples[0]['processed_sample']
            for key, value in first_sample.items():
                val_str = str(value)
                if len(val_str) > 50: 
                    val_str = val_str[:50] + "..."
                logger.info(f"  {key}: {type(value).__name__} - {val_str}")
        
        return str(output_file)

if __name__ == "__main__":
    """
    ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•  ë•Œ ìƒ˜í”Œ ë°ì´í„° ì €ì¥
    ì‚¬ìš©ë²•: 
    - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python utils/datasets/rag_cot_en_ko.py
    - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: python utils/datasets/rag_cot_en_ko.py --streaming
    """
    import sys
    
    try:
        print("RAG-COT-En_KO ë°ì´í„°ì…‹ ìƒ˜í”Œ ì €ì¥ ì‹œì‘...")
        
        # ëª…ë ¹í–‰ ì¸ìˆ˜ í™•ì¸
        use_streaming = "--streaming" in sys.argv
        use_small_sample = "--small" in sys.argv
        
        if use_streaming:
            print("ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì‚¬ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )")
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ì§€ë§Œ í•„í„°ë§ ìƒëµ
            dataset = RagCotEnKoDataset(split="train", streaming=True, max_samples=50)
        elif use_small_sample:
            print("âš¡ ì†Œê·œëª¨ ìƒ˜í”Œ ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)")
            # ì‘ì€ ìƒ˜í”Œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            dataset = RagCotEnKoDataset(split="train", max_samples=1000)
        else:
            print("ğŸ“Š ê¸°ë³¸ ëª¨ë“œ (ì „ì²´ ë°ì´í„°ì…‹)")
            # ê¸°ë³¸ ëª¨ë“œ
            dataset = RagCotEnKoDataset(split="train", max_samples=5000)  # 5ì²œê°œë¡œ ì œí•œ
        
        # JSON ì €ì¥
        json_file = dataset.save_random_samples(num_samples=10, output_format="json")
        print(f"[SUCCESS] JSON ìƒ˜í”Œ ì €ì¥ ì™„ë£Œ: {json_file}")
        
        # CSV ì €ì¥
        csv_file = dataset.save_random_samples(num_samples=5, output_format="csv")
        print(f"[SUCCESS] CSV ìƒ˜í”Œ ì €ì¥ ì™„ë£Œ: {csv_file}")
        
        # ë°ì´í„° êµ¬ì¡° í™•ì¸ìš© ìƒ˜í”Œ ì¶œë ¥
        print("\n=== ìƒ˜í”Œ ë°ì´í„° í™•ì¸ ===")
        samples = dataset.get_random_samples(n=3)
        for i, (question, answer, context) in enumerate(samples, 1):
            print(f"\n[ìƒ˜í”Œ {i}]")
            print(f"ì§ˆë¬¸: {question[:100]}...")
            print(f"ë‹µë³€: {answer[:100]}...")  
            print(f"ì»¨í…ìŠ¤íŠ¸: {context[:100]}...")
        
        print(f"\nâœ… ë°ì´í„° í™•ì¸:")
        print("   - datasets/samples í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print("\nğŸ’¡ ì‚¬ìš©ë²•:")
        print("   - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: python utils/datasets/rag_cot_en_ko.py --streaming")
        print("   - ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸: python utils/datasets/rag_cot_en_ko.py --small")
        
    except Exception as e:
        print(f"[ERROR] ì—ëŸ¬ ë°œìƒ: {e}")
        logger.error(f"Failed to save samples: {e}")