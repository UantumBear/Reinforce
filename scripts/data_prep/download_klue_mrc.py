"""
@ê²½ë¡œ : scripts/data_prep/download_klue_mrc.py
@ì„¤ëª… : Hugging Faceì—ì„œ KLUE MRC ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ëœë¤ ìƒ˜í”Œì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤
@ëª…ë ¹ì–´ : python scripts/data_prep/download_klue_mrc.py

- 26.02.07 êµ¬ì¡° ë³€ê²½ í›„ í…ŒìŠ¤íŠ¸ í•´ë³´ì§€ ì•Šì•„ì„œ í…ŒìŠ¤íŠ¸ í•„ìš”
"""
import random
import json
import os
import sys
from pathlib import Path
from typing import Tuple, List
from datasets import load_dataset

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ ê²½ë¡œ ì„¤ì • (import ì‹œì—ëŠ” ì‹¤í–‰ ì•ˆ ë¨)
if __name__ == "__main__":
    # Python ê²½ë¡œ ì„¤ì • (PYTHONPATH=. íš¨ê³¼)
    project_root = Path(__file__).resolve().parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    # í™˜ê²½ë³€ìˆ˜ë„ ì„¤ì • (ì„ íƒì‚¬í•­)
    os.environ['PYTHONPATH'] = str(project_root)
from utils.log.logging import logger

class KlueMrcDataset:
    """KLUE MRC ë°ì´í„°ì…‹ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, split: str = "train"):
        """
        ë°ì´í„°ì…‹ ì´ˆê¸°í™” ë° ë¡œë“œ
        
        Args:
            split (str): ì‚¬ìš©í•  ë°ì´í„° ë¶„í•  ('train', 'validation' ë“±)
        """
        logger.info(f"Loading KLUE MRC dataset (split={split})...")
        
        # HuggingFace ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì •ë³´ ë¡œê¹…
        dataset_name = "klue"
        config_name = "mrc"
        logger.info(f"Dataset identifier: {dataset_name}")
        logger.info(f"Dataset config: {config_name}")
        logger.info(f"HuggingFace URL: https://huggingface.co/datasets/{dataset_name}")
        
        try:
            # ë°ì´í„°ì…‹ ë¡œë“œ ì „ ìºì‹œ ê²½ë¡œ í™•ì¸ (datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ë³„ í˜¸í™˜ì„±)
            try:
                from datasets.utils.file_utils import HF_CACHE_HOME
                cache_dir = HF_CACHE_HOME
            except ImportError:
                try:
                    from datasets import config
                    cache_dir = config.HF_DATASETS_CACHE
                except (ImportError, AttributeError):
                    import os
                    cache_dir = os.path.expanduser("~/.cache/huggingface/datasets")
            
            logger.info(f"Local cache directory: {cache_dir}")
            
            # lmqg/qg_koquad ë°ì´í„°ì…‹ ì‚¬ìš© (ì§ˆë¬¸ ìƒì„±/ë‹µë³€ìš©ìœ¼ë¡œ ì •ì œëœ ë²„ì „)
            # self.dataset = load_dataset("lmqg/qg_koquad", split=split, trust_remote_code=True)
            self.dataset = load_dataset(dataset_name, config_name, split=split)
            #  trust_remote_code=True : ë°ì´í„°ì…‹ ë‚´ë¶€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í—ˆìš© (ë³´ì•ˆ ì—ëŸ¬ í•´ê²°)

            logger.info(f"Successfully loaded {len(self.dataset)} samples.")
            
            # ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ì •ë³´ ë¡œê¹…
            if hasattr(self.dataset, 'info'):
                logger.info(f"Dataset info: {self.dataset.info}")
            if hasattr(self.dataset, 'builder_name'):
                logger.info(f"Builder name: {self.dataset.builder_name}")
            if hasattr(self.dataset, 'config_name'):
                logger.info(f"Config name: {self.dataset.config_name}")
        except Exception as e:
            logger.error(f"Failed to load dataset: {e}")
            raise e
    
    def get_cache_info(self):
        """ë°ì´í„°ì…‹ ìºì‹œ ì •ë³´ë¥¼ ë°˜í™˜"""
        try:
            from datasets.utils.file_utils import HF_CACHE_HOME
            import os
            
            cache_info = {
                "cache_home": HF_CACHE_HOME,
                "cache_exists": os.path.exists(HF_CACHE_HOME),
                "dataset_name": "klue",
                "config_name": "mrc"
            }
            
            if hasattr(self.dataset, 'cache_files'):
                cache_info["cache_files"] = self.dataset.cache_files
            
            logger.info(f"Cache info: {cache_info}")
            return cache_info
            
        except Exception as e:
            logger.error(f"Failed to get cache info: {e}")
            return {}
    
    def get_cache_info(self):
        """ë°ì´í„°ì…‹ ìºì‹œ ì •ë³´ë¥¼ ë°˜í™˜"""
        try:
            from datasets.utils.file_utils import HF_CACHE_HOME
            import os
            
            cache_info = {
                "cache_home": HF_CACHE_HOME,
                "cache_exists": os.path.exists(HF_CACHE_HOME),
                "dataset_name": "klue",
                "config_name": "mrc"
            }
            
            if hasattr(self.dataset, 'cache_files'):
                cache_info["cache_files"] = self.dataset.cache_files
            
            logger.info(f"Cache info: {cache_info}")
            return cache_info
            
        except Exception as e:
            logger.error(f"Failed to get cache info: {e}")
            return {}

    def get_random_samples(self, batch_size: int = 1) -> list:
        """
        ë°ì´í„°ì…‹ì—ì„œ ëœë¤í•˜ê²Œ ì§ˆë¬¸, ì •ë‹µ, ì§€ë¬¸ ìŒì„ ë°˜í™˜
        
        Args:
            batch_size (int): ë°˜í™˜í•  ìƒ˜í”Œ ê°œìˆ˜
        
        Returns:
            list: [(ì§ˆë¬¸1, ì •ë‹µ1, ì§€ë¬¸1), (ì§ˆë¬¸2, ì •ë‹µ2, ì§€ë¬¸2), ...] í˜•íƒœì˜ íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.dataset:
            raise ValueError("Dataset not loaded.")
        
        samples = []
        
        for _ in range(batch_size):
            sample = random.choice(self.dataset)
            
            # KLUE MRC ì‹¤ì œ êµ¬ì¡°: {'question': ..., 'answers': {'text': [...], 'answer_start': [...]}, 'context': ...}
            question = sample['question']
            context = sample['context']  # ì§€ë¬¸ ì¶”ê°€
            
            # answers í•„ë“œì—ì„œ ì²« ë²ˆì§¸ ë‹µë³€ ì¶”ì¶œ
            answers = sample['answers']
            if isinstance(answers['text'], list) and len(answers['text']) > 0:
                answer = answers['text'][0]
            else:
                answer = str(answers['text'])
            
            samples.append((question, answer, context))
        
        return samples

    def get_fixed_sample(self, index: int = 0) -> Tuple[str, str, str]:
        """
        íŠ¹ì • ì¸ë±ìŠ¤ì˜ ìƒ˜í”Œ ë°˜í™˜ (ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            index (int): ë°ì´í„°ì…‹ ë‚´ íŠ¹ì • ì¸ë±ìŠ¤ (ê¸°ë³¸ê°’: 0)
            
        Returns:
            Tuple[str, str, str]: (ì§ˆë¬¸, ì •ë‹µ, ì§€ë¬¸)
        """
        sample = self.dataset[index]
        
        # get_random_samplesì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
        question = sample['question']
        context = sample['context']  # ì§€ë¬¸ ì¶”ê°€
        
        # answers í•„ë“œì—ì„œ ì²« ë²ˆì§¸ ë‹µë³€ ì¶”ì¶œ
        answers = sample['answers']
        if isinstance(answers['text'], list) and len(answers['text']) > 0:
            answer = answers['text'][0]
        else:
            answer = str(answers['text'])
        
        return question, answer, context

    def save_random_samples(self, num_samples: int = 10, output_format: str = "json") -> str:
        """
        ëœë¤ ìƒ˜í”Œë“¤ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë°ì´í„° êµ¬ì¡° í™•ì¸
        
        Args:
            num_samples (int): ì €ì¥í•  ìƒ˜í”Œ ê°œìˆ˜ (ê¸°ë³¸: 10ê°œ)
            output_format (str): ì¶œë ¥ í˜•ì‹ ("json" ë˜ëŠ” "csv", ê¸°ë³¸: "json")
            
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if not self.dataset:
            raise ValueError("Dataset not loaded.")
        
        # samples í´ë” ìƒì„±
        samples_dir = Path("datasets/samples")
        samples_dir.mkdir(parents=True, exist_ok=True)
        
        # ëœë¤ ìƒ˜í”Œ ìˆ˜ì§‘
        samples = []
        for i in range(min(num_samples, len(self.dataset))):
            sample = random.choice(self.dataset)
            # ì›ë³¸ êµ¬ì¡° ê·¸ëŒ€ë¡œ ì €ì¥
            samples.append({
                "index": i,
                "raw_sample": dict(sample),  # ì›ë³¸ ë°ì´í„° êµ¬ì¡°
                "sample_keys": list(sample.keys()),  # ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ë“¤
            })
        
        if output_format.lower() == "json":
            # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
            output_file = samples_dir / "klue_mrc_samples.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(samples, f, ensure_ascii=False, indent=2)
        
        elif output_format.lower() == "csv":
            # CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥
            import csv
            output_file = samples_dir / "klue_mrc_samples.csv"
            
            if samples:
                # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ í‚¤ë“¤ë¡œ í—¤ë” ìƒì„±
                fieldnames = ['index', 'sample_keys'] + list(samples[0]['raw_sample'].keys())
                
                with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    for sample in samples:
                        row = {
                            'index': sample['index'],
                            'sample_keys': str(sample['sample_keys'])
                        }
                        # raw_sampleì˜ ëª¨ë“  í•„ë“œ ì¶”ê°€
                        for key, value in sample['raw_sample'].items():
                            row[key] = str(value) if not isinstance(value, (str, int, float)) else value
                        writer.writerow(row)
        else:
            raise ValueError("output_format must be 'json' or 'csv'")
        
        logger.info(f"Saved {len(samples)} samples to {output_file}")
        logger.info(f"Sample structure analysis:")
        if samples:
            logger.info(f"  Available keys: {samples[0]['sample_keys']}")
            logger.info(f"  Sample data types:")
            for key, value in samples[0]['raw_sample'].items():
                logger.info(f"    {key}: {type(value).__name__} - {str(value)[:100]}...")
        
        return str(output_file)


if __name__ == "__main__":
    """
    ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•  ë•Œ ìƒ˜í”Œ ë°ì´í„° ì €ì¥
    ì‚¬ìš©ë²•: python utils/datasets/klue_mrc.py
    """
    try:
        print("KLUE MRC ë°ì´í„°ì…‹ ìƒ˜í”Œ ì €ì¥ ì‹œì‘...")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = KlueMrcDataset(split="train")
        
        # JSON í˜•ì‹ìœ¼ë¡œ 10ê°œ ìƒ˜í”Œ ì €ì¥
        json_file = dataset.save_random_samples(num_samples=10, output_format="json")
        print(f"âœ… JSON ìƒ˜í”Œ ì €ì¥ ì™„ë£Œ: {json_file}")
        
        # CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (ì„ íƒì‚¬í•­)
        csv_file = dataset.save_random_samples(num_samples=5, output_format="csv")
        print(f"âœ… CSV ìƒ˜í”Œ ì €ì¥ ì™„ë£Œ: {csv_file}")
        
        print("\nğŸ“‹ ë°ì´í„° êµ¬ì¡° í™•ì¸ì„ ìœ„í•´ ì €ì¥ëœ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!")
        print("   - JSON íŒŒì¼: ìƒì„¸í•œ êµ¬ì¡° ë¶„ì„ìš©")
        print("   - CSV íŒŒì¼: ì—‘ì…€ì—ì„œ ì‰½ê²Œ í™•ì¸ ê°€ëŠ¥")
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        logger.error(f"Failed to save samples: {e}")